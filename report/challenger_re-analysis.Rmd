---
title: "W271 Group Lab 1"
subtitle: "Investigating the 1986 Space Shuttle Challenger Accident"
author: "Please fill in with your names."
output: bookdown::pdf_document2
fontsize: 11pt
geometry: margin=1in
---

\newpage

```{=tex}
\begin{abstract} 
This report will, indeed, be abstract. No, instead, describe your goals your approach, and what you learn.
In this study, we are investigating the effect of physical condition such as temperature and pressure have on the odds of O Rings failure on a space shuttle. 
\end{abstract}
```
# Introduction

## Research question

How do temperature and pressure affect the odds of o-ring failures on a shuttle launch?

Do physical conditions, in term of temperature and pressure, affect the odds of O-Rings failures on a shuttle?

# Data (20 points)

```{asis, echo=FALSE}
**Complete the following task. In your final submission, please remove this question prompt so that your report reads as a report. The Data Section of this report is worth 20 points.**

-   Conduct a thorough EDA of the data set.

    -   This should include both graphical and tabular analysis as taught in this course.
    -   Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals.

-   This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.
```

```{r load packages for homework 2, echo=FALSE, message=FALSE}
# install.packages("MASS")
# install.packages("ggplot2")
# install.packages("sandwich")
# install.packages("stargazer")
# install.packages("mcprofile")
# install.packages("Hmisc")
# install.packages("patchwork")
# install.packages("car")
library(tidyverse)
library(patchwork)
library(gridExtra)
library(Hmisc)
# multinomial regression
library(nnet)
# car pacakge for testing
library(car)
library(sandwich)
library(knitr)
library(stargazer)
library(mcprofile)
library(Hmisc)
library(gridExtra)

theme_set(theme_minimal())
knitr::opts_chunk$set(tidy.opts = list(width_cutoff = 100), tidy = TRUE, message = FALSE, warning = FALSE)
rm(list = ls())
```

```{r read-challenger-data, echo=FALSE, message=FALSE}
challenger <- read_csv("../data/raw/challenger.csv")
```

```{r tabular-EDA, echo=FALSE, message=FALSE}
# glimpse(challenger)
#tab <- table(summary(challenger))
#tab
#describe(challenger)
#nulls <- sum(is.na(challenger))
#cat(nulls, "missing values in the data set.")
```

```{r Analysis-of-Response-Variable, echo=FALSE, message=FALSE, fig.align='center', out.width="50%", fig.cap="Numer of Obserserved O-ring Failures"}
challenger %>%
  ggplot(aes(x = O.ring, y = ..prop.., group = 1)) +
  geom_bar(fill = "DarkBlue", color = "black") +
  geom_text(stat = "count", aes(label = ..count..), vjust = -1) +
  xlab("Number of Failed O Rings") +
  ylab("Proportion") +
  ylim(0, 1)
```


## Description
```{asis, echo=FALSE}
**Complete the following task. In your final submission, please remove this question prompt so that your report reads as a report.**

-   Describe the data that you are using. How is this data generated, what is the sampling process that brought it to your availability. If it is helpful, you might describe the population (i.e. the Random Variables) that exist and how samples are produced from these random variables.
-   The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authors' concerns about independence.
```

The data for this analysis is from the 23 pre-Challenger launches of the space shuttle. It contains 5 columns: Flight, Temp, Pressure, O.ring, Number. Flight is a unique identifier representing each flight observed. Number represents the total number of O-rings in each spacecraft (6 for each spacecraft). O.ring is a random variable that denotes the number of O-rings failed at each launch. Lastly, Temp and Pressure are numerical values for the physical condition measurements taken at each launch. 

There are 23 launches in the data and no missing values in any of the features in the data set. Temperature has 16 distinct values with a range between 53 to 81 and a mean of 69.57. Pressure has 3 distinct values with a range between 50 to 200 and a mean of 152. Both temperature and pressure records are full integer value. In terms of the response variable, number of failed O-rings, the majority of flights have 0 failed O-ring (16 out of 23). For the failed launch, there are 5 observations with 1 failed O-ring and 2 observations with 2 failed O-rings.
 
In the original study, the researchers treated each of the 6 O-rings as independent for each of the 23 launches. In other words, the failure of one O-ring do not cause any of the other O-ring to fail. This assumption is necessary to satisfy the binomial model's assumption of independence. The binomial model also assumes that each of the O-rings has equal probability to be damaged by either temperature or pressure. But when considering each O-ring as independent, the researcher might not account for the fact that a single failed O-ring could cause the other o-ring to fail and the entire launch to fail. While careful evaluation of the potential dependency between each O-ring is necessary, we will continue our analysis follow the independence assumption.

## Key Features
```{r Scatterplot of O-Rings, echo=FALSE, message=FALSE}
scatter <- challenger %>%
  ggplot(aes(x = Temp, y = O.ring)) +
  geom_point() +
  coord_quickmap() +
  ylab("Number Of O-rings Failed") +
  xlab("Temperature") 
```

```{r Distribution Plots, echo=FALSE, message=FALSE,fig.align='center', out.width="50%", fig.cap="O-ring Failure Distribution Plot for Temperature (Top) and Pressure (Bottom)"}
p1 <- challenger %>%
  ggplot(aes(x = Temp)) +
  geom_density(aes(
    y = ..density..,
    color = factor(O.ring), fill = factor(O.ring)
  ), alpha = 0.2) +
  ggtitle("Distribution of Temp") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  xlab("Temp") +
  ylab("Density")

p2 <- challenger %>%
  ggplot(aes(x = Pressure)) +
  geom_density(aes(
    y = ..density..,
    color = factor(O.ring), fill = factor(O.ring)
  ), alpha = 0.2) +
  ggtitle("Distribution of Pressure") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  xlab("Pressure") +
  ylab("Density")
grid.arrange(p1, p2)

```

```{r Box Plots, echo=FALSE, message=FALSE, fig.align='center', out.width="50%", fig.cap="O-ring Failure Box Plot for Temperature (Top) and Pressure (Bottom)"}
p3 <- challenger %>%
  ggplot(aes(factor(O.ring), Temp)) +
  geom_boxplot(aes(fill = factor(O.ring))) +
  coord_flip() +
  ggtitle("Temp by Number of Oring Incidents") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("Temp") +
  xlab("O.ring")

p4 <- challenger %>%
  ggplot(aes(factor(O.ring), Pressure)) +
  geom_boxplot(aes(fill = factor(O.ring))) +
  coord_flip() +
  ggtitle("Pressure by Number of Oring Incidents") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("Pressure") +
  xlab("O.ring")

grid.arrange(p3, p4)
```

From the above study, we noted two features, temperature and pressure, that most likely affect the odds of O-rings failure on a space shuttle.From the distribution and box plot, the number of O-rings failed has a higher distribution at the lower temperature.

In term of pressure, the distribution plots showed that 0 failed O-rings at both high and low pressure while 2 failed O-rings showed a uniform distribution. However, it is noted that this plot does somewhat show that one O-rings failed has higher distribution in the higher pressure cases vs. the lower pressure cases. In the following study, we will be using both temperature and pressure as explanatory variable to model the odds of each individual O-ring fail during the launch. Our goal is to shed some light into the statistical significance of each of this feature on O-ring failure.

# Analysis

## Reproducing Previous Analysis (10 points)
```{asis, echo=FALSE}
**Your analysis should address the following two questions. In your final submission, please remove this question prompt so that your report reads as a report.**

1.  Estimate the logistic regression model that the authors present in their report -- include the variables as linear terms in the model. Evaluate, using likelihood ratio tests, the statistical significance of each explanatory variable in the model. Evaluate, using the context and data understanding that you have created in the **Data** section of this report, the practical significance of each explanatory variable in the model.
```

Our baseline model is a binomial logistic regression model replicates what the authors present in their report. 

```{r model-1-temp-and-pressure, echo=FALSE, message=FALSE}
# Since there are 6 O-rings on a shuttle
# We define the failure rates as number of O Rings Failed out of 6 O Rings
# create a binomial logistic regression model to evaluate the odds of O ring failure
mod1 <- glm(
  formula = cbind(O.ring, Number - O.ring) ~ Temp + Pressure,
  family = binomial(link = "logit"),
  data = challenger
)
```

```{r model-1-stargazer, results = "asis", echo=FALSE}
stargazer(mod1,
  type = "latex", omit.stat = "f",
  covariate.labels = c("Temperature", "Pressure", "(Intercept)"),
  report = ("vc*p"),
  star.cutoffs = c(0.05, 0.01, 0.001),
  title = "The Estimated Relationship Between O-Rings Failure and Temperature/Pressure",
  dep.var.caption = "Output Variable: Log Odds of O-Rings Failure",
  dep.var.labels = "",
  column.sep.width = "-8pt"
)
```

```{r model-1-OR, echo = FALSE, message=FALSE}
# Odds ratio for model 1
OR <- round(exp(10*mod1$coefficients), 3)[2]
OR
```

For the first model, we perform a logistic regression on the odds of O-ring failed response using both temperature and pressure.

The estimated model is $y = 2.520 - 0.98*Temp + 0.008*Pressure$. 

Using a Wald test, the model return a p-value of 0.029 for temperature and 0.270 for pressure. Since the p-value for Temperature is below the 95% confident level, Temperature is statistical significant to estimate the O-Rings failure. The p-value for pressure is above this level and is considered not statistically significant. Additionally, from the fitted model, we see that temperature have a coefficient of -0.098, indicating that an increase in temperature decrease the log odd of an O-ring failure. While pressure is not statistically significant, the positive coefficients suggested that an increase in pressure increase the log odd of an O-ring failure. These observations are aligned with what we saw from the distribution and box plot charts in our EDA analysis. We took this analysis further by computing the Odd Ratios for each estimated coefficients for a 10 units increase in temperature or pressure. Here, the odds of an O-ring failure is `r OR` times as likely, or about 63% less likely, for a 10 unit increase in temperature. The pressure yield an Odd Ratio of 1, which indicate that an increase in pressure have similar effect on the odds of an O-ring failure as no increase in pressure. 

```{asis, echo=FALSE}
2.  Dalal, Fowlkes, and Hoadley (1989) chose to remove `pressure` from the model based on their likelihood ratio tests. Critically evaluate, using your test results and understanding of the question and data, whether `pressure` should be included in the model, or instead, `pressure` should not be included in the model. Your report needs to make a determination, argue why it is most appropriate choice, and make note of how (if at all) the model results are affected by the choice of including or excluding `pressure`.
```


``` {r LRT-for-model-1, message=FALSE}
Anova(mod1, test = "LR")
```

In order to test for the significance of Pressure, we use the Likelihood Ratio Test for each explanatory variable. For this test, we stated a Null hypothesis that $\beta_2 = 0$, or Pressure does not add significant explanatory power for the odds of O-ring failure, on top of the Temp in the model. 

From the test, we obtained a p-value for a Chi-squared distribution of 0.0228 for Temperature and 0.2145 for pressure. Like the Wald test, the p-value for Pressure is above the 95% confident level, alpha=0.05, so there is not enough evidence to reject the Null hypothesis.

Considering both test results, we conclude that Pressure does not add much explanatory power to the model. Excluding it from the model is especially recommended in this case, considering the sample size of 23 observations, the drop in the degree of freedom is 5% when we add one additional explanatory variable. AIC score also decreases with one fewer variable and lower AIC is better. Keeping the model simple and easy to interpret would also help with the further study. 

## Confidence Intervals (20 points)

```{asis, echo=FALSE}
No matter what you determined about using or dropping `pressure`, for this section begin by considering the simplified model $logit(\pi) = \beta_0 + \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure.
Complete the following:

1.  Estimate the logistic regression model.
2.  Determine if a quadratic term is needed in the model for the temperature in this model.
3.  Construct two plots:
4.  $\pi$ vs. Temp; and,
5.  Expected number of failures vs. Temp.

Specific requirements for these plots:

-   Use a temperature range of 31° to 81° on the x-axis even though the minimum temperature in the data set was 53°.\
-   Include the 95% Wald confidence interval bands for $\pi$ on the plot. Describe, in your analysis of these plots, why the bands much wider for lower temperatures than for higher temperatures?
```

After defining Temperature as our explanatory variable in the previous section, an anova test determined whether a quadratic term is needed in the logistic regression model.

```{r Estimate Logistic Regression with Temperature Effects, echo = FALSE}
# Estimate the logistic regression model
mod2 <- glm(
  formula = cbind(O.ring, Number - O.ring) ~ Temp,
  family = binomial(link = "logit"),
  data = challenger
)
mod3 <- glm(
  formula = cbind(O.ring, Number - O.ring) ~ Temp + I(Temp^2),
  family = binomial(link = "logit"),
  data = challenger
)
```

```{r model23-stargazer, results = "asis", echo=FALSE}
stargazer(mod2, mod3,
  type = "latex", omit.stat = "f",
  report = ("vc*p"),
  star.cutoffs = c(0.05, 0.01, 0.001),
  covariate.labels = c("Temperature", "Squared Temperature", "(Intercept)"),
  title = "The Estimated Relationship Between O-Rings Failure and Temperature Effects",
  dep.var.caption = "Output Variable: Log Odds of O-Rings Failure",
  dep.var.labels = "",
  column.sep.width = "-8pt"
)
```

```{r Determine if quadratic term is needed, echo = FALSE}
anova(mod2, mod3, test = "Chisq")
```

The anova test yielded a p-value of 0.48, which means that there is little to no evidence that a quadratic term is statistically significant. Therefore we fail to reject the null hypothesis that `mod2` is equal to `mod3`. Therefore, we will move forward with `mod2` without the quadratic term.

```{r Pi-vs-Temp-prediction-and-CI, echo = FALSE, message = FALSE}
mod.beta <- mod2$coefficients
# calculate true wald confidence interval at temperature
wald.CI.true.coverage <- function(mod, x_value) {
  alpha <- 0.5
  predict.data <- data.frame(Temp = x_value)
  linear.pred <- predict(object = mod, newdata = predict.data, type = "link", se = TRUE)
  CI.lin.pred <- linear.pred$fit + qnorm(p = c(alpha / 2, 1 - alpha / 2)) * linear.pred$se.fit
  CI.pi <- exp(CI.lin.pred) / (1 + exp(CI.lin.pred))
  wald.df <- round(data.frame(x_value, lower = CI.pi[1], upper = CI.pi[2]), 4)
  return(wald.df)
}
x_seq <- seq(31, 81, by = 1)
wald.CI.true.matrix <- matrix(data = NA, nrow = length(x_seq), ncol = 3)
counter <- 1
for (x_value in x_seq) {
  wald.df2 <- wald.CI.true.coverage(mod2, x_value)
  wald.CI.true.matrix[counter, ] <- c(x_value, wald.df2$lower, wald.df2$upper)
  counter <- counter + 1
}
# check that the matrix is correct
# wald.CI.true.matrix[1:5, ]
```

```{r plot-pi-vs-temp, echo=FALSE, fig.align='center',fig.align='center', out.width="60%", fig.cap="Temperature Effect on the Probability of O-ring Failure"}
# Reproduce the graph overlaying the same result from the linear model as a comparison
curve(
  expr = exp(mod.beta[1] + mod.beta[2] * x) / (1 + exp(mod.beta[1] + mod.beta[2] * x)),
  xlim = c(31, max(challenger$Temp)),
  ylim = c(0, 1),
  col = "blue",
  sub = expression(pi == frac(e^(beta[0] + beta[1] * Temp), 1 + e^(beta[0] + beta[1] * Temp))), # this writes the formula below the curve  # TODO: can we above formula floating?
  main = "Probability of O-ring Failure vs. Temp",
  xlab = "",
  ylab = expression(pi),
)
par(new = TRUE)
# plot lower and upper bounds to the existing plot
for (matrix_ind in c(2, 3)) {
  lines(
    x = x_seq,
    y = wald.CI.true.matrix[, matrix_ind],
    ylim = c(0, 1),
    type = "l",
    lty = "dotted",
    col = "red"
  )
}
```


```{r Graph-Expected-number-of-failures-vs-Temp, echo=FALSE, fig.align='center', out.width="60%", fig.cap="Temperature Effect Expected Number of O-ring Failure"}
curve(
  expr = 6 * exp(mod.beta[1] + mod.beta[2] * x) / (1 + exp(mod.beta[1] + mod.beta[2] * x)),
  xlim = c(31, max(challenger$Temp)),
  ylim = c(0, 6),
  col = "blue",
  xlab = expression(Temp), ylab = "Expected Number of Failures",
  main = expression(pi == 6*frac(e^(beta[0] + beta[1] * x), 1 + e^(beta[0] + beta[1] * x)))
)
par(new = TRUE)
```

From the above chart, odds of O-Rings failure declines as temperature increases, as $\beta_1$ is negative. The confidence intervals are wider for temperatures lower than 50F and narrower for temperatures greater than 65F. The estimated interval for temperature equal to 30F is about (1,6); this illustrates that we have high variability for the expected number of incidents. The reason for wider confidence interval for lower than 50F is because the minimum temperature in the data set was 53F. The forecast error for the below-50F is larger.

```{asis, echo=FALSE}
3.  The temperature was 31° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.
```


```{r Prob-at-31-degrees, echo=FALSE, message = FALSE, align='center'}
# Compute the Probability at T=31 degree
params <- dim(summary(mod2)$coefficients)[1]
predict.data <- matrix(
  data = c(1, 31),
  nrow = 1,
  ncol = params
)
linear.combo <- mcprofile(object = mod2, CM = predict.data)
mroz_logit_profile <- confint(linear.combo, level = 0.95)
mroz_logit_ci <- exp(mroz_logit_profile$confint) / (1 + exp(mroz_logit_profile$confint))
mroz_logit_estimate <- exp(mroz_logit_profile$estimate) / (1 + exp(mroz_logit_profile$estimate))
result.profile_lr <- data.frame(
  estimate = mroz_logit_estimate,
  lower = mroz_logit_ci$lower,
  upper = mroz_logit_ci$upper
)
result.profile_lr
```

To apply the inference procedures, we assume asympotic properties of a large sample size, independence of each O-Ring failure and each failure has equal probability. At 31F, the model estimes 81.8% probability of O-Rings failure, with 95% confidence interval of 14.2% and 99%.

## Bootstrap Confidence Intervals (30 points)
```{asis, echo=FALSE}

Rather than relying on asymptotic properties, consider using a parametric bootstrap, as did Dalal, Fowlkes and Hoadley.
To do this:

1.  Simulate a large number of data sets (n = 23 for each) by re-sampling with replacement from the data.
2.  Estimate a model for each dataset.
3.  Compute the effect at a specific temperature of interest.

To produce a confidence interval, the authors used the 0.05 and 0.95 observed quantiles from the simulated distribution as their 90% confidence interval limits.

Using the parametric bootstrap, compute 90% confidence intervals separately at each integer temperature between 10° and 100° Fahrenheit.

In this section, you should describe your process, justify such a process, and present your results in a way that is compelling for your reader.
```

```{r resample with replacement, echo = FALSE, message=FALSE}
# TODO: create a for loop to do this,
# for each data set:
# for each model from the resulting data set:
# compute the probability at each integer temeprature value (10 to 100)
# compute the CI for the probability
temp <- NULL
pi.hat <- NULL
CI.lower <- NULL
CI.upper <- NULL
for (i in 1:100) {
  challenger_resampled <- NULL
  # creating a resampled data from the original dataset with replacement
  challenger_resampled <- challenger[sample(nrow(challenger), size = 23, replace = TRUE), ]
  # construct a model from the new resampled dataset
  mod_resample <- glm(
    formula = cbind(O.ring, Number - O.ring) ~ Temp,
    family = binomial(link = "logit"),
    data = challenger_resampled
  )
  output <- NULL
  conf.lower <- NULL
  conf.upper <- NULL
  tprime <- NULL
  # run the model though to obtain the output
  for (t in 10:100) {
    new.data <- data.frame("Temp" = t)
    pred.prob <- predict(mod_resample, newdata = new.data, type = "link", se = TRUE)
    # compute probability and CI
    pi <- exp(pred.prob$fit) / (1 + exp(pred.prob$fit))
    linear.Wald.CI <- pred.prob$fit + qnorm(p = c(0.05, 0.95)) * pred.prob$se
    Wald.CI <- exp(linear.Wald.CI) / (1 + exp(linear.Wald.CI))
    # append data to array
    tprime <- c(tprime, t)
    output <- c(output, pi)
    conf.lower <- c(conf.lower, Wald.CI[1])
    conf.upper <- c(conf.upper, Wald.CI[2])
  }
  # append each bootstrap run
  temp <- c(temp, tprime)
  pi.hat <- c(pi.hat, output)
  CI.lower <- c(CI.lower, conf.lower)
  CI.upper <- c(CI.upper, conf.upper)
}
bootstrap_ouptuts <- data.frame(
  temp = temp, pi.hat = pi.hat,
  lower = CI.lower, upper = CI.upper
)
agg_tbl <- bootstrap_ouptuts %>%
  group_by(temp) %>%
  summarise(
    pi.hat = mean(pi.hat),
    lower = mean(lower),
    upper = mean(upper)
  )
```


```{r Bootstrap-Probability-Distribution-and-QQ-plots-T-31, echo=FALSE, message=FALSE, fig.align='center', out.width="60%", fig.cap="Bootstrap Distribution at T=$31^o$ F"}
# sliced the data on 1 temperature unit
# we chose 31 here since that was what the Challenger launch at
t <- 31
sliced_df <- bootstrap_ouptuts[bootstrap_ouptuts$temp == t, ]
# Draw two plots next to each other
par(mfrow = c(1, 2))
zs <- seq(0, 1, 0.01)
hist_ <- function(x, ...) {
  hist(x, breaks = 30, xlab = "Z", ylab = "", yaxt = "n", freq = FALSE, ...)
  # lines(zs, normal_density, type = "l", col = "red", lwd = 2)
}
hist_(as.numeric(sliced_df$pi.hat), main = "T = 31 Gaussian Distribution", xlim = c(0, 1))
qqnorm(as.numeric(sliced_df$pi.hat))
qqline(as.numeric(sliced_df$pi.hat), col = "blue", lwd = 2)
```

```{r Bootstrap-Probability-Distribution-and-QQ-plots-T-53, echo=FALSE, message=FALSE, fig.align='center', out.width="60%", fig.cap="Bootstrap Distribution at T=$53^o$ F"}
# sliced the data on 1 temperature unit
# we chose 53 here due to the lowest launch reference temperature
t <- 53
sliced_df <- bootstrap_ouptuts[bootstrap_ouptuts$temp == t, ]
# Draw two plots next to each other
par(mfrow = c(1, 2))
zs <- seq(0, 1, 0.01)
hist_ <- function(x, ...) {
  hist(x, breaks = 30, xlab = "Z", ylab = "", yaxt = "n", freq = FALSE, ...)
  # lines(zs, normal_density, type = "l", col = "red", lwd = 2)
}
hist_(as.numeric(sliced_df$pi.hat), main = "T = 53 Gaussian Distribution", xlim = c(0, 1))
qqnorm(as.numeric(sliced_df$pi.hat))
qqline(as.numeric(sliced_df$pi.hat), col = "blue", lwd = 2)
```

Statistical bootstrapping is a common method use when we want to generate some statistics with reasonable confidence. Since we are not able to perform launching experiment for large N, bootstrapping allows us to simulation such a process. In this study, we performed 100-folds bootstrap of the challenger datataset so that we can obtain 100 probability of failure at each temperature ranging from $10^o$ to $100^o$ Fahrenheit. It is worth nothing here that at each iteration, we're sampling 23 observations from the original challenger dataset with replacement so that we do not have the same set of values in each iteration. After the dataset is sampled, we used the new re-sampled dataset to fit a model for the log Odd of O-ring Failure using only temperature as explanatory variable. By fitting a new model for each re-sampled dataset, we are able to obtain slightly different coefficient that would ultimately yield a slightly different prediction at each iteration. Finally, we record the probability of an O-ring failure as well as the 90% Wald confidence interval limits given an integer temperature between $10^o$ to $100^o$ Fahrenheit using each resampled model. 

From the result obtained, we sliced the data on two temperatures values $31^o$ F and $53^o$ F to reference the temperature at which the Challenger spaceship launch failed and the lowest launch temperature. From the two sliced data, we generated a histogram of our 100 bootstrap sampled as well as the Q-Q plot to illustrate the predicted probability deviations versus the theoretical Gaussian distribution. Here, we are able to see that due to having literally no launches at  $31^o$ F, the failure probability distribution is sparse but shown to be aggregate at high probability region between 0.7-1.0. In contrast to  $53^o$ F, the failure probability distribution is more Gaussian-like and aggregate at low probability region between 0.0-0.4. The Q-Q plots for both temperatures values further support this by showing the predicted failure probability versus the theoretical failure probability at each quantiles. Noted that the $53^o$ F Q-Q plot shows an almost 1-1 match between the two quantiled probability while there is a clear deviation near the tail ends for the $31^o$ F Q-Q plot. 

While it is safe to say that if we're able to replicate the launch measurement at each temperature values 100 more times, we'll have a better idea of the probability of failure at each temperature. Using the bootstrapping method, we're able to somewhat show the effect of launch temperature of odds of an O-ring failure. In short, the temperature at which the Challenger space craft launch at, $31^o$ F has a really high probability of O-ring failure. Delaying the launch to a warmer temperature would have helped the outcomes

## Alternative Specification (10 points)

```{asis, echo=FALSE}
With the same set of explanatory variables in your final model, estimate a linear regression model.
Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions.
Would you use the linear regression model or binary logistic regression in this case?
Explain why.
```

```{r linear model, results = "asis", echo=FALSE, message=FALSE}
model.linear <- lm(formula = I(O.ring/Number) ~ Temp, data = challenger)
stargazer(model.linear,
  type = "latex", omit.stat = "f",
  report = ("vc*p"),
  star.cutoffs = c(0.05, 0.01, 0.001),
  title = "Linear Effect of O-Rings Failure and Temperature"
)
```

```{r checking linear model Homeoskedasticity, echo=FALSE, message=FALSE, fig.align='center', out.width="60%", fig.cap="Linear Regression Residuals"}
par(mfrow=c(2,2)) # init 4 charts in 1 panel
plot(model.linear)
```
Since pressure was not statistically significant, we only use temperature as an explanatory variable to a linear regression to model the probability of failure, defined as number of O-rings failed divided by the total number of O-rings. With this model, we obtained a p-value of 0.013 for temperature, which is lowered than the stated 95% Wald confidence level, or alpha 0.05 and visualized by 1 asterisk. The negative coefficient for temperature suggests that at an increase in pressure decreases the probability of failure. 

While that is all well and good, performing a linear regression in this aspect is not ideal because the probability of O.ring failure can fall outside of 0 and 1. Additionally, the linear model assumes that the probability is linearly related to the explanatory variable, temperature in this case, that is not necessarily true because in the original dataset, two launches with the same temperature can have different number of O-ring failure (ex: launch 16 and launch 21). Further analysis into the linear effect model also show that it violates the homoskedasticity assumption, noted as the residual versus fitted value dipped below 0 for a brief period of time. For those reasons, if we wanted to model this properly, a logistic regression would have to be use.


# Conclusions (10 points)
```{asis, echo=FALSE}
Interpret the main result of your preferred model in terms of both odds and probability of failure.
Summarize this result with respect to the question(s) being asked and key takeaways from the analysis.
```

```{r}
OR = round(exp(coef(mod2)),4)[2]
```


In conclusion, our final model is a binomial logistic regression model with o-ring success as the dependent variable and temperature as the explanatory variable. The table above is the odds ratio of having a 1 unit increase from that corresponding temperature. For example, the odds of an o-ring failing increases by `r OR` times for every 1-unit increase in temperature. 

Our explanatory variable analysis involved anova tests comparing models with combinations of Temperature, Pressure, and Temperature-squared. The results demonstrate that Model 3, with Temperature as the only explanatory variable for O.Ring failure, is the most statistically significant model. In this model, the probability of o-ring failure decreases as temperature increases, and the confidence interval also dramatically decreases when the temperature is greater than 65 degrees compared to less than 65 degrees. This observation is reinforced through parametric bootstrapping. With 100 iterations of bootstrapping and 23 observations per iteration, the results showed that at the lowest temperature of 31F, the failure probability distribution is sparse but aggregates at a high probability region between 0.7 and 1.0 while at 53F, the failure probability distribution is Gaussian-like and aggregates at a low probability region between 0.0 and 0.4. Therefore, delaying the launch to a warmer temperature decreases the likelihood of o-ring failure. Finally, we considered a linear regression model as an alternative specifications against the binomial model, but doing so would violate the assumption of an identically independent distribution. 




